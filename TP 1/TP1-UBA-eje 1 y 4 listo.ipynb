{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Dh8MkXaG-c9Y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Big Data y Machine Learning (UBA) -  2025\n",
    "\n",
    "## Trabajo Práctico 1: Jugando con APIs y WebScraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhBlm6mZ-c9e"
   },
   "source": [
    "### Reglas de formato y presentación\n",
    "- El trabajo debe estar debidamente documentado comentado (utilizando #) para que tanto los docentes como sus compañeros puedan comprender el código fácilmente.\n",
    "\n",
    "- El mismo debe ser completado en este Jupyter Notebook y entregado como tal, es decir en un archivo .ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEjGaa4U-c9g"
   },
   "source": [
    "### Fecha de entrega:\n",
    "Viernes 4 de Abril a las 13:00 hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9TU2y7E-c9h"
   },
   "source": [
    "### Modalidad de entrega\n",
    "- Al finalizar el trabajo práctico deben hacer un último <i>commit</i> en su repositorio de GitHub llamado “Entrega final del tp”. \n",
    "- Asegurense de haber creado una carpeta llamada TP1. Este Jupyter Notebook y el correspondiente al TP1 deben estar dentro de esa carpeta.\n",
    "- También deben enviar el link de su repositorio -para que pueda ser clonado y corregido- a mi correo 25RO35480961@campus.economicas.uba.ar. Usar de asunto de email <i>\"Big Data - TP 1 - Grupo #\"</i> y nombrar el archivo <i>\"TP1_Grupo #\"</i> donde # es el número de grupo que le fue asignado.\n",
    "- La última versión en el repositorio es la que será evaluada. Por lo que es importante que: \n",
    "    - No envien el correo hasta no haber terminado y estar seguros de que han hecho el <i>commit y push</i> a la versión final que quieren entregar. \n",
    "    - No hagan nuevos <i>push</i> despues de haber entregado su versión final. Esto generaría confusión acerca de que versión es la que quieren que se les corrija.\n",
    "- En resumen, la carpeta del repositorio debe incluir:\n",
    "    - El codigo\n",
    "    - Un documento Word (Parte A) donde esten las figuras y una breve descripción de las mismas.\n",
    "    - El excel con los links webscrappeados (Parte B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXbrPraa-c9i"
   },
   "source": [
    "#### Ejercicio 1 - Jugando con APIs\n",
    "Usando la API del Banco Mundial [link](https://wbdata.readthedocs.io/en/stable/) , obtener dos series de indicadores para dos paises a elección en una consulta de búsqueda. Pueden buscar serie de indicadores de su interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wbdata in c:\\users\\wally\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: appdirs<2.0,>=1.4 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from wbdata) (1.4.4)\n",
      "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from wbdata) (2.2.1)\n",
      "Requirement already satisfied: cachetools<6.0.0,>=5.3.2 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from wbdata) (5.3.3)\n",
      "Requirement already satisfied: dateparser<2.0.0,>=1.2.0 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from wbdata) (1.2.1)\n",
      "Requirement already satisfied: decorator<6.0.0,>=5.1.1 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from wbdata) (5.1.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.0 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from wbdata) (2.32.3)\n",
      "Requirement already satisfied: shelved-cache<0.4.0,>=0.3.1 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from wbdata) (0.3.1)\n",
      "Requirement already satisfied: tabulate<0.9.0,>=0.8.5 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from wbdata) (0.8.10)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from dateparser<2.0.0,>=1.2.0->wbdata) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2024.2 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from dateparser<2.0.0,>=1.2.0->wbdata) (2025.2)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from dateparser<2.0.0,>=1.2.0->wbdata) (2024.9.11)\n",
      "Requirement already satisfied: tzlocal>=0.2 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from dateparser<2.0.0,>=1.2.0->wbdata) (5.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.0->wbdata) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.0->wbdata) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.0->wbdata) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.0->wbdata) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.0->dateparser<2.0.0,>=1.2.0->wbdata) (1.16.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\wally\\anaconda3\\lib\\site-packages (from tzlocal>=0.2->dateparser<2.0.0,>=1.2.0->wbdata) (2023.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\wally\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wbdata #instalo las librerías \n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wbdata\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package wbdata:\n",
      "\n",
      "NAME\n",
      "    wbdata - wbdata: A wrapper for the World Bank API\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    cache\n",
      "    client\n",
      "    dates\n",
      "    fetcher\n",
      "\n",
      "FUNCTIONS\n",
      "    get_default_client() -> wbdata.client.Client\n",
      "        Get the default client\n",
      "\n",
      "VERSION\n",
      "    1.0.0\n",
      "\n",
      "FILE\n",
      "    c:\\users\\wally\\anaconda3\\lib\\site-packages\\wbdata\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wbdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicadores = {'NY.GDP.PCAP.CD': 'PIB per cápita (USD corrientes)','SP.DYN.LE00.IN':'Esperanza de vida al nacer (años)'} #elijo indicadores\n",
    "\n",
    "\n",
    "data = wbdata.get_dataframe(indicadores, country=['BRA','ARG'])\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset[\"date\"] = df_reset[\"date\"].astype(int)      #hago esto ya que no me muestra las primeras 10 filas de brasil así que lo configuro por años\n",
    "df_brasil = df_reset[(df_reset[\"country\"] == \"Brazil\") & \n",
    "                      (df_reset[\"date\"] >= 2014) & \n",
    "                      (df_reset[\"date\"] <= 2023)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                PIB per cápita (USD corrientes)  \\\n",
      "country   date                                    \n",
      "Argentina 2023                     14187.482725   \n",
      "          2022                     13935.681111   \n",
      "          2021                     10738.017922   \n",
      "          2020                      8535.599380   \n",
      "          2019                      9955.974787   \n",
      "          2018                     11752.799892   \n",
      "          2017                     14532.500931   \n",
      "          2016                     12699.962314   \n",
      "          2015                     13679.626498   \n",
      "          2014                     12233.144412   \n",
      "\n",
      "                Esperanza de vida al nacer (años)  \n",
      "country   date                                     \n",
      "Argentina 2023                                NaN  \n",
      "          2022                             76.064  \n",
      "          2021                             75.390  \n",
      "          2020                             75.892  \n",
      "          2019                             77.284  \n",
      "          2018                             76.999  \n",
      "          2017                             76.833  \n",
      "          2016                             76.308  \n",
      "          2015                             76.760  \n",
      "          2014                             76.755  \n",
      "   country  date  PIB per cápita (USD corrientes)  \\\n",
      "64  Brazil  2023                     10294.866681   \n",
      "65  Brazil  2022                      9281.333344   \n",
      "66  Brazil  2021                      7972.536650   \n",
      "67  Brazil  2020                      7074.193783   \n",
      "68  Brazil  2019                      9029.833267   \n",
      "69  Brazil  2018                      9300.661649   \n",
      "70  Brazil  2017                     10080.509282   \n",
      "71  Brazil  2016                      8836.286526   \n",
      "72  Brazil  2015                      8936.196618   \n",
      "73  Brazil  2014                     12274.993969   \n",
      "\n",
      "    Esperanza de vida al nacer (años)  \n",
      "64                                NaN  \n",
      "65                             73.425  \n",
      "66                             72.750  \n",
      "67                             74.009  \n",
      "68                             75.338  \n",
      "69                             75.109  \n",
      "70                             74.827  \n",
      "71                             74.442  \n",
      "72                             74.332  \n",
      "73                             74.306  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.head(10))  # Muestra las primeras 10 filas de argentina ( da la casualidad que justo hay datos en estos años y no NaN lo que permite comparar )\n",
    "print(df_brasil)   # para que me muestre las de brasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2 - Repaso de Pandas\n",
    "Realicen una estadistica descriptiva de ambas series de indicadores comparando los dos países."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolver acá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3 - Practicando con Matplotlib\n",
    "Armen dos gráficos distintos usando la librería Matplotlib (repasen Clase 4). Uno programandolo con el estilo *pyplot* y otro gráfico de estilo *orientada a objetos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolver acá estilo pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolver acá estilo orientado-objetos \n",
    "# Tip: aprovechar este estilo de programar una figura para hacerlo más lindo \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4\n",
    "De la página de noticias del [diario La Nación](https://www.lanacion.com.ar/) o cualquier diario que les interese, utilicen herramientas de web scraping para obtener los **links** de las noticias de la portada. Guarden los links obtenidos en un dataframe y expórtenlo a un archivo de excel.\n",
    "\n",
    "Nota 1: es posible que logren obtener los links a las noticias sin el dominio: \"https://www.lanacion.com.ar/\". De ser así, concatenen el dominio a la ruta del link obtenido, tal que se obtenga un link al que se pueda acceder. Es decir, que las cadenas de caracteres finales tendrán la forma: https://www.lanacion.com.ar/*texto_obtenido*)\n",
    "\n",
    "Nota 2: junto con su entrega, adjunten una captura de la página de noticias al momento de correr su código. Eso servirá al momento de la corrección para verificar que los links obtenidos hacen referencia a las noticias de ese día y hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\wally\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\wally\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\wally\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\wally\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\wally\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\wally\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\wally\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\wally\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\wally\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.4 MB 1.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.8/9.4 MB 1.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.0/9.4 MB 1.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.3/9.4 MB 1.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.6/9.4 MB 1.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.8/9.4 MB 1.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.1/9.4 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.4/9.4 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 2.6/9.4 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 2.9/9.4 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 3.1/9.4 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.4/9.4 MB 1.2 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 3.7/9.4 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 3.9/9.4 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 4.2/9.4 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 4.5/9.4 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 4.5/9.4 MB 1.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 4.7/9.4 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.0/9.4 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 5.2/9.4 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 5.5/9.4 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 5.8/9.4 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 6.0/9.4 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 6.3/9.4 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 6.6/9.4 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 6.8/9.4 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.1/9.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.3/9.4 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 7.6/9.4 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 7.9/9.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.1/9.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.4/9.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.4 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-25.3.0 outcome-1.3.0.post0 selenium-4.30.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas openpyxl \n",
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han guardado 239 enlaces en el archivo noticias_lanacion_20250403_145925.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# URL del diario La Nación\n",
    "url = \"https://www.lanacion.com.ar/\"\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificar que la solicitud fue exitosa\n",
    "if response.status_code == 200:\n",
    "    # Parsear el contenido HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los links de las noticias (ajustar la clase según la estructura de la página)\n",
    "    links = []\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if href.startswith(\"/\") and \"#\" not in href:  # Esto es para evitar enlaces internos irrelevantes\n",
    "            full_link = url + href.lstrip(\"/\")\n",
    "            links.append(full_link)\n",
    "        elif href.startswith(\"http\") and \"lanacion.com.ar\" in href:\n",
    "            links.append(href)\n",
    "    \n",
    "    # Eliminar duplicados- esto es para que no me muestre los link dos veces.\n",
    "    links = list(set(links))\n",
    "    \n",
    "    # Crear DataFrame con los links\n",
    "    df = pd.DataFrame({'Links': links})\n",
    "    \n",
    "    # Exportar a un archivo Excel\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    excel_filename = f\"noticias_lanacion_{timestamp}.xlsx\"\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    \n",
    "    print(f\"Se han guardado {len(links)} enlaces en el archivo {excel_filename}.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "TP1 - Parte 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
